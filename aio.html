<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to artificial neural networks in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to artificial neural networks in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to artificial neural networks in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to artificial neural networks in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-xrays.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-visualisation.html">2. Visualisation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-data_prep.html">3. Data preparation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-create_net.html">4. Neural networks</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-train_eval.html">5. Training and evaluation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-explainability.html">6. Explainability</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="discuss.html">Discussion</a></li>
<li><a href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-xrays"><p>Content from <a href="01-xrays.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-05-22 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/01-xrays.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What kinds of conditions can be detected in chest X-rays?</li>
<li>How does pleural effusion appear on a chest X-ray?</li>
<li>How can chest X-ray data be used to train a machine learning
model?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand what chest X-rays show and how they are used in
diagnosis.</li>
<li>Recognize pleural effusion as a condition visible on chest
X-rays.</li>
<li>Gain familiarity with the NIH ChestX-ray dataset.</li>
<li>Load and explore a balanced set of labeled chest X-rays for model
training.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="chest-x-rays">Chest X-rays<a class="anchor" aria-label="anchor" href="#chest-x-rays"></a>
</h2>
<hr class="half-width">
<p>Chest X-rays are frequently used in healthcare to view the heart,
lungs, and bones of patients. On an X-ray, broadly speaking, bones
appear white, soft tissue appears grey, and air appears black. The
images can show details such as:</p>
<ul>
<li>Lung conditions, for example pneumonia, emphysema, or air in the
space around the lung.</li>
<li>Heart conditions, such as heart failure or heart valve
problems.</li>
<li>Bone conditions, such as rib or spine fractures</li>
<li>Medical devices, such as pacemaker, defibrillators and catheters.
X-rays are often taken to assess whether these devices are positioned
correctly.</li>
</ul>
<p>In recent years, organisations like the <a href="https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community" class="external-link">National
Institutes of Health</a> have released large collections of X-rays,
labelled with common diseases. The goal is to stimulate the community to
develop algorithms that might assist radiologists in making diagnoses,
and to potentially discover other findings that may have been
overlooked.</p>
<p>The following figure is from a study by <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf" class="external-link">Xiaosong
Wang et al</a>. It illustrates eight common diseases that the authors
noted could be be detected and even spatially-located in front chest
x-rays with the use of modern machine learning algorithms.</p>
<figure><img src="fig/wang_et_al.png" alt="Chest X-ray diseases" width="600" class="figure mx-auto d-block"></figure><div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What are some possible challenges when working with real chest X-ray
data?<br>
Think about issues related to the data itself (e.g. image quality,
labels), as well as how the data might be used in a clinical or machine
learning setting.</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>Possible challenges include:</li>
</ol>
<ul>
<li>
<strong>Label noise</strong>: Labels are often derived from
radiology reports using automated tools, and may not be 100%
accurate.</li>
<li>
<strong>Ambiguity in diagnosis</strong>: Even expert radiologists
may disagree on the interpretation of an image.</li>
<li>
<strong>Variability in image quality</strong>: X-rays may be over-
or under-exposed, blurry, or taken from non-standard angles.</li>
<li>
<strong>Presence of confounders</strong>: Images may include
pacemakers, tubes, or other devices that distract or bias a model.</li>
<li>
<strong>Data imbalance</strong>: In real-world datasets, some
conditions (like pleural effusion) may be much less common than
others.</li>
<li>
<strong>Generalization</strong>: A model trained on one dataset may
not perform well on data from a different hospital or population.</li>
</ul>
<p>These challenges highlight why data curation, domain expertise, and
robust validation are critical in medical machine learning.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="pleural-effusion">Pleural effusion<a class="anchor" aria-label="anchor" href="#pleural-effusion"></a>
</h2>
<hr class="half-width">
<p>Thin membranes called “pleura” line the lungs and facilitate
breathing. Normally there is a small amount of fluid present in the
pleura, but certain conditions can cause excess build-up of fluid. This
build-up is known as pleural effusion, sometimes referred to as “water
on the lungs”.</p>
<p>Causes of pleural effusion vary widely, ranging from mild viral
infections to serious conditions such as congestive heart failure and
cancer. In an upright patient, fluid gathers in the lowest part of the
chest, and this build up is visible to an expert.</p>
<p>For the remainder of this lesson, we will develop an algorithm to
detect pleural effusion in chest X-rays. Specifically, using a set of
chest X-rays labelled as either “normal” or “pleural effusion”, we will
train a neural network to classify unseen chest X-rays into one of these
classes.</p>
</section><section><h2 class="section-heading" id="loading-the-dataset">Loading the dataset<a class="anchor" aria-label="anchor" href="#loading-the-dataset"></a>
</h2>
<hr class="half-width">
<p>The data that we are going to use for this project consists of 350
“normal” chest X-rays and 350 X-rays that are labelled as showing
evidence pleural effusion. These X-rays are a subset of the public NIH
ChestX-ray dataset.</p>
<blockquote>
<p>Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri,
Ronald Summers, ChestX-ray8: Hospital-scale Chest X-ray Database and
Benchmarks on Weakly-Supervised Classification and Localization of
Common Thorax Diseases, IEEE CVPR, pp. 3462-3471, 2017</p>
</blockquote>
<p>Let’s begin by loading the dataset.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># The glob module finds all the pathnames matching a specified pattern</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># If your dataset is compressed, unzip with:</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># !unzip chest_xrays.zip</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># Define folders containing images</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>data_path <span class="op">=</span> os.path.join(<span class="st">"chest_xrays"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>effusion_path <span class="op">=</span> os.path.join(data_path, <span class="st">"effusion"</span>, <span class="st">"*.png"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>normal_path <span class="op">=</span> os.path.join(data_path, <span class="st">"normal"</span>, <span class="st">"*.png"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Create list of files</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>effusion_list <span class="op">=</span> glob(effusion_path)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>normal_list <span class="op">=</span> glob(normal_path)</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of cases with pleural effusion: '</span>, <span class="bu">len</span>(effusion_list)) </span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of normal cases: '</span>, <span class="bu">len</span>(normal_list))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of cases with pleural effusion:  350
Number of normal cases:  350</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Chest X-rays are widely used to identify lung, heart, and bone
abnormalities.</li>
<li>Pleural effusion is a condition where excess fluid builds up around
the lungs, visible in chest X-rays.</li>
<li>Large public datasets like the NIH ChestX-ray dataset enable the
development of machine learning models to detect disease.</li>
<li>In this lesson, we will train a neural network to classify chest
X-rays as either “normal” or showing pleural effusion.</li>
<li>We begin by loading a balanced dataset of labeled chest X-ray
images.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-visualisation"><p>Content from <a href="02-visualisation.html">Visualisation</a></p>
<hr>
<p>Last updated on 2025-05-21 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/02-visualisation.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does a chest X-ray with pleural effusion differ from a normal
X-ray?</li>
<li>How is an image represented and manipulated as a NumPy array?</li>
<li>What steps are needed to prepare images for machine learning?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Visually compare chest X-rays with and without pleural
effusion.</li>
<li>Understand how images are represented as arrays in NumPy.</li>
<li>Learn to load and preprocess image data for use in machine
learning.</li>
<li>Practice displaying image slices and understanding their pixel-level
structure.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="visualising-the-x-rays">Visualising the X-rays<a class="anchor" aria-label="anchor" href="#visualising-the-x-rays"></a>
</h2>
<hr class="half-width">
<p>In the previous section, we set up a dataset comprising 700 chest
X-rays. Half of the X-rays are labelled “normal” and half are labelled
as “pleural effusion”. Let’s take a look at some of the images.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># cv2 is openCV, a popular computer vision library</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt </span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="kw">def</span> plot_example(example, label, loc):</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    image <span class="op">=</span> cv2.imread(example)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    im <span class="op">=</span> ax[loc].imshow(image)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    title <span class="op">=</span> <span class="ss">f"Class: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>example<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    ax[loc].set_title(title)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>fig.set_size_inches(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Plot a "normal" record</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>plot_example(random.choice(normal_list), <span class="st">"Normal"</span>, <span class="dv">0</span>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Plot a record labelled with effusion</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>plot_example(random.choice(effusion_list), <span class="st">"Effusion"</span>, <span class="dv">1</span>)</span></code></pre>
</div>
<figure><img src="fig/example_records.png" alt="Example X-rays" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="can-we-detect-effusion">Can we detect effusion?<a class="anchor" aria-label="anchor" href="#can-we-detect-effusion"></a>
</h2>
<hr class="half-width">
<p>Run the following code to flip a coin to select an x-ray from our
collection.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Effusion or not?"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># flip a coin</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>coin_flip <span class="op">=</span> random.choice([<span class="st">"Effusion"</span>, <span class="st">"Normal"</span>])</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="cf">if</span> coin_flip <span class="op">==</span> <span class="st">"Normal"</span>:</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    fn <span class="op">=</span> random.choice(normal_list)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    fn <span class="op">=</span> random.choice(effusion_list)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># plot the image</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(fn)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>plt.imshow(image)</span></code></pre>
</div>
<p>Show the answer:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Jupyter doesn't allow us to print the image until the cell has run,</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># so we'll print in a new cell.</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The answer is: </span><span class="sc">{</span>coin_flip<span class="sc">}</span><span class="ss">!"</span>)</span></code></pre>
</div>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<p>Use the coin-flip X-ray viewer to classify 10 chest X-rays.</p>
<ul>
<li>Record whether you think each image is “Normal” or “Effusion”.</li>
<li>After viewing the answer, mark whether you were correct.</li>
<li>Calculate your accuracy: correct predictions ÷ total
predictions.</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Your accuracy is the fraction of correct predictions (e.g. 6 out of
10 = 60%).<br>
Remember the number! Later, we’ll use it as baseline for evaluating a
neural network.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="how-does-a-computer-see-an-image">How does a computer see an image?<a class="anchor" aria-label="anchor" href="#how-does-a-computer-see-an-image"></a>
</h2>
<hr class="half-width">
<p>Consider an image as a matrix in which the value of each pixel
corresponds to a number that determines a tone or color. Let’s load one
of our images:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>file_idx <span class="op">=</span> <span class="dv">56</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>example <span class="op">=</span> normal_list[file_idx]</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(example)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(512, 512, 3)</code></pre>
</div>
<p>Here we see that the image has 3 dimensions. The first dimension is
height (512 pixels) and the second is width (also 512 pixels). The
presence of a third dimension indicates that we are looking at a color
image (“RGB”, or Red, Green, Blue).</p>
<p>For more detail on image representation in Python, take a look at the
<a href="https://datacarpentry.org/image-processing/" class="external-link">Data Carpentry
course on Image Processing with Python</a>. The following image is
reproduced from the <a href="https://datacarpentry.org/image-processing/03-skimage-images/index.html" class="external-link">section
on Image Representation</a>.</p>
<figure><img src="fig/chair-layers-rgb.png" alt="RGB image" width="600" class="figure mx-auto d-block"></figure><p>For simplicity, we’ll instead load the images in greyscale. A
greyscale image has two dimensions: height and width. Greyscale images
have only one channel. Most greyscale images are 8 bits per channel or
16 bits per channel. For a greyscale image with 8 bits per channel, each
value in the matrix represents a tone between black (0) and white
(255).</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(example, cv2.IMREAD_GRAYSCALE)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(512, 512)</code></pre>
</div>
<p>Let’s briefly display the matrix of values, and then see how these
same values are rendered as an image.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Print a 10 by 10 chunk of the matrix</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="bu">print</span>(image[<span class="dv">35</span>:<span class="dv">45</span>, <span class="dv">30</span>:<span class="dv">40</span>])</span></code></pre>
</div>
<figure><img src="fig/greyscale_example_numpy.png" alt="Example greyscale numpy array" width="400" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Plot the same chunk as an image</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>plt.imshow(image[<span class="dv">35</span>:<span class="dv">45</span>, <span class="dv">30</span>:<span class="dv">40</span>], cmap<span class="op">=</span><span class="st">'gray'</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">255</span>)</span></code></pre>
</div>
<figure><img src="fig/greyscale_example.png" alt="Example greyscale image" width="400" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="image-pre-processing">Image pre-processing<a class="anchor" aria-label="anchor" href="#image-pre-processing"></a>
</h2>
<hr class="half-width">
<p>In the next section, we’ll be building and training a model. Let’s
prepare our data for the modelling phase. For convenience, we’ll begin
by loading all of the images and corresponding labels and assigning them
to a list.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># create a list of effusion images and labels</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>dataset_effusion <span class="op">=</span> [cv2.imread(fn, cv2.IMREAD_GRAYSCALE) <span class="cf">for</span> fn <span class="kw">in</span> effusion_list]</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>label_effusion <span class="op">=</span> np.ones(<span class="bu">len</span>(dataset_effusion))</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co"># create a list of normal images and labels</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>dataset_normal <span class="op">=</span> [cv2.imread(fn, cv2.IMREAD_GRAYSCALE) <span class="cf">for</span> fn <span class="kw">in</span> normal_list]</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>label_normal <span class="op">=</span> np.zeros(<span class="bu">len</span>(dataset_normal))</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co"># Combine the lists</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>dataset <span class="op">=</span> dataset_effusion <span class="op">+</span> dataset_normal</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>labels <span class="op">=</span> np.concatenate([label_effusion, label_normal])</span></code></pre>
</div>
<div class="section level3">
<h3 id="downsampling">Downsampling<a class="anchor" aria-label="anchor" href="#downsampling"></a>
</h3>
<p>X-ray images are often high resolution, which can be useful for
detailed clinical interpretation. However, for training a machine
learning model, especially in an educational or prototype setting, using
smaller images can reduce:</p>
<ul>
<li>Memory usage: smaller images require less RAM and storage.</li>
<li>Computation time: smaller images train faster.</li>
<li>Overfitting risk: smaller inputs reduce the number of parameters and
complexity.</li>
</ul>
<p>For these reasons, we will downsample each image from 512×512 pixels
to 256×256 pixels. This still preserves important features (like fluid
in the lungs) while reducing the computational cost.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># Downsample the images from (512,512) to (256,256)</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>dataset <span class="op">=</span> [cv2.resize(img, (<span class="dv">256</span>,<span class="dv">256</span>)) <span class="cf">for</span> img <span class="kw">in</span> dataset]</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># Check the size of the reshaped images</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="bu">print</span>(dataset[<span class="dv">0</span>].shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(256, 256)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="standardisation">Standardisation<a class="anchor" aria-label="anchor" href="#standardisation"></a>
</h3>
<p>Before training a model, it’s important to scale input data. A common
approach is standardization, which adjusts the pixel values so that each
image has zero mean and unit variance. This helps neural networks learn
more effectively by ensuring that the input data is centered and
scaled.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Standardize the data</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co"># For each image: subtract the mean and divide by the standard deviation</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dataset)):</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>  dataset[i] <span class="op">=</span> (dataset[i] <span class="op">-</span> np.mean(dataset[i])) <span class="op">/</span> np.std(dataset[i])</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="reshaping">Reshaping<a class="anchor" aria-label="anchor" href="#reshaping"></a>
</h3>
<p>Finally, we’ll convert our dataset from a list to an array. We are
expecting it to be (700, 256, 256), representing 700 images (350
effusion and 350 normal), each with dimensions 256×256.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>dataset <span class="op">=</span> np.asarray(dataset, dtype<span class="op">=</span>np.float32)</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Matrix Dimensions: </span><span class="sc">{</span>dataset<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(700, 256, 256)</code></pre>
</div>
<div id="exercise-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-1" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<p>Pick any grayscale image from your dataset (hint:
<code>dataset[idx]</code>) and inspect the following:</p>
<ol style="list-style-type: upper-alpha">
<li>What is the new shape of the image array?<br>
</li>
<li>What are the mean and standard deviation of the pixel values?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>Shape after resizing: <code>dataset[0].shape</code> (256, 256)</li>
<li>The mean is 0 (<code>dataset[0].mean()</code>) and the standard
deviation is 1 (<code>dataset[0].std()</code>)</li>
</ol>
</div>
</div>
</div>
</div>
<p>We could plot the images by indexing them on <code>dataset</code>,
e.g., we can plot the first image in the dataset with:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>vals <span class="op">=</span> dataset[idx].flatten()</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>plt.imshow(dataset[idx], cmap<span class="op">=</span><span class="st">'gray'</span>, vmin<span class="op">=</span><span class="bu">min</span>(vals), vmax<span class="op">=</span><span class="bu">max</span>(vals))</span></code></pre>
</div>
<figure><img src="fig/final_example_image.png" alt="Example greyscale image" width="400" class="figure mx-auto d-block"></figure><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>X-ray images can be loaded and visualized using Python libraries
like OpenCV and NumPy.</li>
<li>Images are stored as 2D arrays (grayscale) or 3D arrays (RGB).</li>
<li>Visual inspection helps us understand how disease features appear in
imaging data.</li>
<li>Preprocessing steps like resizing and standardization prepare data
for machine learning.</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-03-data_prep"><p>Content from <a href="03-data_prep.html">Data preparation</a></p>
<hr>
<p>Last updated on 2025-05-21 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/03-data_prep.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Why do we divide data into training, validation, and test sets?</li>
<li>What is data augmentation, and why is it useful for small
datasets?</li>
<li>How can random transformations help improve model performance?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Split the dataset into training, validation, and test sets.</li>
<li>Prepare image and label arrays in the format expected by
TensorFlow.</li>
<li>Apply basic image augmentation to increase training data
diversity.</li>
<li>Understand the role of data preprocessing in model
generalization.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="partitioning-the-dataset">Partitioning the dataset<a class="anchor" aria-label="anchor" href="#partitioning-the-dataset"></a>
</h2>
<hr class="half-width">
<p>Before training our model, we must split the dataset into three
subsets:</p>
<ul>
<li>
<strong>Training set</strong>: Used to train the model.</li>
<li>
<strong>Validation set</strong>: Used to tune parameters and monitor
for overfitting.</li>
<li>
<strong>Test set</strong>: Used for final performance
evaluation.</li>
</ul>
<p>This separation helps ensure that our model generalizes to new,
unseen data.</p>
<p>To ensure reproducibility, we set a <code>random_state</code>, which
controls the random number generator and guarantees the same split every
time we run the code.</p>
<p>TensorFlow expects image input in the format:</p>
<p><code>[batch_size, height, width, channels]</code></p>
<p>So we’ll also expand our image and label arrays to include the final
channel dimension (grayscale images have 1 channel).</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># Reshape arrays to include a channel dimension:</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># [height, width] → [height, width, 1]</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>dataset_expanded <span class="op">=</span> dataset[..., np.newaxis]</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>labels_expanded <span class="op">=</span> labels[..., np.newaxis]</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># Create training and test sets (85% train, 15% test)</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>dataset_train, dataset_test, labels_train, labels_test <span class="op">=</span> train_test_split(</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    dataset_expanded, labels_expanded, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Further split training set to create validation set (15% of remaining data)</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>dataset_train, dataset_val, labels_train, labels_val <span class="op">=</span> train_test_split(</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>    dataset_train, labels_train, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"No. images, x_dim, y_dim, colors) (No. labels, 1)</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train: </span><span class="sc">{</span>dataset_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>labels_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation: </span><span class="sc">{</span>dataset_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>labels_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test: </span><span class="sc">{</span>dataset_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>labels_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>No. images, x_dim, y_dim, colors) (No. labels, 1)

Train: (505, 256, 256, 1), (505, 1)
Validation: (90, 256, 256, 1), (90, 1)
Test: (105, 256, 256, 1), (105, 1)</code></pre>
</div>
</section><section><h2 class="section-heading" id="data-augmentation">Data Augmentation<a class="anchor" aria-label="anchor" href="#data-augmentation"></a>
</h2>
<hr class="half-width">
<p>Our dataset is small, which increases the risk of
<strong>overfitting</strong>, when a model learns patterns specific to
the training set but performs poorly on new data.</p>
<p><strong>Data augmentation</strong> helps address this by creating
modified versions of the training images on-the-fly using random
transformations. This teaches the model to become more robust to
variations it might encounter in real-world data.</p>
<p>We can use <code>ImageDataGenerator</code> to define the types of
augmentation to apply.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># Define what kind of transformations we would like to apply</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># such as rotation, crop, zoom, position shift, etc</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    width_shift_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    height_shift_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">False</span>)</span></code></pre>
</div>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Modify the <code>ImageDataGenerator</code> to include one or more of
the following:</li>
</ol>
<ul>
<li><code>rotation_range=20</code></li>
<li><code>zoom_range=0.2</code></li>
<li><code>horizontal_flip=True</code></li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>Here’s an example:</li>
</ol>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">True</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Now let’s view the effect on our X-rays!:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># specify path to source data</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>path <span class="op">=</span> os.path.join(<span class="st">"chest_xrays"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>batch_size<span class="op">=</span><span class="dv">5</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>val_generator <span class="op">=</span> datagen.flow_from_directory(</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>        path, color_mode<span class="op">=</span><span class="st">"rgb"</span>,</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>        target_size<span class="op">=</span>(<span class="dv">256</span>, <span class="dv">256</span>),</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size)</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="kw">def</span> plot_images(images_arr):</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>    axes <span class="op">=</span> axes.flatten()</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>    <span class="cf">for</span> img, ax <span class="kw">in</span> <span class="bu">zip</span>(images_arr, axes):</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>        ax.imshow(img.astype(<span class="st">'uint8'</span>))</span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>    plt.show()</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>augmented_images <span class="op">=</span> [val_generator[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size)]</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>plot_images(augmented_images)</span></code></pre>
</div>
<figure><img src="fig/xray_augmented.png" alt="X-ray augmented" width="1200" class="figure mx-auto d-block"></figure><div id="exercise-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-1" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>How do the new augmentations affect the appearance of the
X-rays?<br>
Can you still tell they are chest X-rays?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>The augmented images may appear rotated, zoomed, or flipped.<br>
While they might look distorted, they remain visually recognizable as
chest X-rays. These augmentations help the model generalize better to
real-world variability.</li>
</ol>
<p>In medical imaging, always consider clinical context. Some
transformations, like left-right flipping, could lead to anatomically
incorrect inputs if not handled carefully.</p>
</div>
</div>
</div>
</div>
<p>Now we have some data to work with, let’s start building our
model.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Data should be split into separate sets for training, validation,
and testing to fairly evaluate model performance.</li>
<li>TensorFlow expects input images in the shape (batch, height, width,
channels).</li>
<li>Data augmentation increases the variety of training data by applying
random transformations.</li>
<li>Augmented images help reduce overfitting and improve generalization
to new data.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-04-create_net"><p>Content from <a href="04-create_net.html">Neural networks</a></p>
<hr>
<p>Last updated on 2025-05-21 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/04-create_net.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a neural network and how is it structured?</li>
<li>What role do activation functions play in learning?</li>
<li>What is the difference between dense and convolutional layers?</li>
<li>Why are convolutional neural networks effective for image
classification?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand the structure and components of a neural network.</li>
<li>Identify the purpose of activation functions and dense layers.</li>
<li>Explain how convolutional layers extract features from images.</li>
<li>Construct a convolutional neural network using TensorFlow and
Keras.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-a-neural-network">What is a neural network?<a class="anchor" aria-label="anchor" href="#what-is-a-neural-network"></a>
</h2>
<hr class="half-width">
<p>An artificial neural network, or just “neural network”, is a broad
term that describes a family of machine learning models that are (very!)
loosely based on the neural circuits found in biology.</p>
<p>The smallest building block of a neural network is a single neuron. A
typical neuron receives inputs (x1, x2, x3) which are multiplied by
learnable weights (w1, w2, w3), then summed with a bias term (b). An
activation function (f) determines the neuron output.</p>
<figure><img src="fig/neuron.png" alt="Neuron" width="600" class="figure mx-auto d-block"></figure><p>From a high level, a neural network is a system that takes input
values in an “input layer”, processes these values with a collection of
functions in one or more “hidden layers”, and then generates an output
such as a prediction. The network has parameters that are systematically
tweaked to allow pattern recognition.</p>
<figure><img src="fig/simple_neural_network.png" alt="Neuron" width="800" class="figure mx-auto d-block"></figure><p>The layers shown in the network above are “dense” or “fully
connected”. Each neuron is connected to all neurons in the preceeding
layer. Dense layers are a common building block in neural network
architectures.</p>
<p>“Deep learning” is an increasingly popular term used to describe
certain types of neural network. When people talk about deep learning
they are typically referring to more complex network designs, often with
a large number of hidden layers.</p>
</section><section><h2 class="section-heading" id="activation-functions">Activation Functions<a class="anchor" aria-label="anchor" href="#activation-functions"></a>
</h2>
<hr class="half-width">
<p>Part of the concept of a neural network is that each neuron can
either be ‘active’ or ‘inactive’. This notion of activity and inactivity
is attempted to be replicated by so called activation functions. The
original activation function was the sigmoid function (related to its
use in logistic regression). This would make each neuron’s activation
some number between 0 and 1, with the idea that 0 was ‘inactive’ and 1
was ‘active’.</p>
<p>As time went on, different activation functions were used. For
example the tanh function (hyperbolic tangent function), where the idea
is a neuron can be active in both a positive capacity (close to 1), a
negative capacity (close to -1) or can be inactive (close to 0).</p>
<p>The problem with both of these is that they suffered from a problem
called <a href="https://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_SSCI_2015/data/7560b423.pdf" class="external-link">model
saturation</a>. This is where very high or very low values are put into
the activation function, where the gradient of the line is almost flat.
This leads to very slow learning rates (it can take a long time to train
models with these activation functions).</p>
<p>One popular activation function that tries to tackle this is the
rectified linear unit (ReLU) function. This has 0 if the input is
negative (inactive) and just gives back the input if it is positive (a
measure of how active it is - the metaphor gets rather stretched here).
This is much faster at training and gives very good performance, but
still suffers model saturation on the negative side. Researchers have
tried to get round this with functions like ‘leaky’ ReLU, where instead
of returning 0, negative inputs are multiplied by a very small
number.</p>
<figure><img src="fig/ActivationFunctions.png" alt="Activation functions" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="convolutional-neural-networks">Convolutional neural networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<hr class="half-width">
<p>Convolutional neural networks (CNNs) are a type of neural network
that especially popular for vision tasks such as image recognition. CNNs
are very similar to ordinary neural networks, but they have
characteristics that make them well suited to image processing.</p>
<p>Just like other neural networks, a CNN typically consists of an input
layer, hidden layers and an output layer. The layers of “neurons” have
learnable weights and biases, just like other networks.</p>
<p>What makes CNNs special? The name stems from the fact that the
architecture includes one or more convolutional layers. These layers
apply a mathematical operation called a “convolution” to extract
features from arrays such as images.</p>
<p>In a convolutional layer, a matrix of values referred to as a
“filter” or “kernel” slides across the input matrix (in our case, an
image). As it slides, values are multiplied to generate a new set of
values referred to as a “feature map” or “activation map”.</p>
<figure><img src="fig/convolution_plotke.gif" alt="2D Convolution Animation by Michael Plotke" width="500" class="figure mx-auto d-block"></figure><p>Filters provide a mechanism for emphasising aspects of an input
image. For example, a filter may emphasise object edges. See <a href="https://setosa.io/ev/image-kernels/" class="external-link">setosa.io</a> for a visual
demonstration of the effect of different filters.</p>
</section><section><h2 class="section-heading" id="max-pooling">Max pooling<a class="anchor" aria-label="anchor" href="#max-pooling"></a>
</h2>
<hr class="half-width">
<p>Convolutional layers often produce large feature maps — one for each
filter. To reduce the size of these maps while retaining the most
important features, we use <strong>pooling</strong>.</p>
<p>The most common type is <strong>max pooling</strong>. It works by
sliding a small window (often 2×2) across the feature map and taking the
<strong>maximum value</strong> in each region. This reduces the
resolution of the feature map (for a 2x2 window by a factor of 2) while
keeping the strongest responses.</p>
<p>For example, if we apply max pooling to the following 4×4 matrix:</p>
<pre><code>[1, 3, 2, 1],
[5, 6, 1, 2],
[4, 2, 9, 8],
[3, 1, 2, 0]</code></pre>
<p>We get this 2×2 output:</p>
<pre><code>[6, 2],
[4, 9]</code></pre>
<p>Each value in the output is the maximum from a 2×2 window in the
input.</p>
<div class="section level3">
<h3 id="why-use-max-pooling">Why use max pooling?<a class="anchor" aria-label="anchor" href="#why-use-max-pooling"></a>
</h3>
<ul>
<li>
<strong>Reduces computation</strong> by shrinking the feature
maps</li>
<li>
<strong>Adds translation tolerance</strong> — the model is less
sensitive to small shifts in the image</li>
<li>
<strong>Keeps the strongest features</strong> while discarding
low-importance details</li>
</ul>
<p>In TensorFlow, max pooling is implemented with the
<code>MaxPool2D()</code> layer. You’ll see it applied multiple times in
our network to gradually reduce the size of the feature maps and focus
on the most prominent features.</p>
</div>
</section><section><h2 class="section-heading" id="dropout">Dropout<a class="anchor" aria-label="anchor" href="#dropout"></a>
</h2>
<hr class="half-width">
<p>When training neural networks, a common problem is overfitting — the
model learns to perform very well on the training data but fails to
generalize to new, unseen examples.</p>
<p>Dropout is a regularization technique that helps reduce overfitting.
During training, dropout temporarily “drops out” (sets to zero) a random
subset of neurons in a layer. This forces the network to learn redundant
representations and prevents it from becoming too reliant on any single
path through the network.</p>
<p>In practice:</p>
<ul>
<li>During training: a random set of neurons is deactivated at each
step.</li>
<li>During inference (prediction), all neurons are used, and the outputs
are scaled accordingly.</li>
</ul>
<p>For example:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dropout</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># Drop 50% of neurons during training</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>x <span class="op">=</span> Dropout(<span class="fl">0.5</span>)(x)</span></code></pre>
</div>
<p>The value 0.5 is the dropout rate — the fraction of neurons to
disable.</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Why is dropout helpful during training?</li>
<li>What effect do you expect from reducing or removing the dropout rate
during training?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li><p>Dropout randomly disables neurons during training, forcing the
network to not rely too heavily on any one path. This helps prevent
overfitting and improves generalization.</p></li>
<li><p>With lower or no dropout, training accuracy may rise faster, but
validation accuracy may stagnate or decline, indicating
overfitting.</p></li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="creating-a-convolutional-neural-network">Creating a convolutional neural network<a class="anchor" aria-label="anchor" href="#creating-a-convolutional-neural-network"></a>
</h2>
<hr class="half-width">
<p>Before training a convolutional neural network, we will first define
its architecture. The architecture we use in this lesson is
intentionally simple. It follows common CNN design principles:</p>
<ul>
<li>Repeated use of small convolutional filters (3×3 or 5×5)</li>
<li>Max pooling to reduce dimensionality</li>
<li>Fully connected layers at the end for classification</li>
</ul>
<p>This architecture is loosely inspired by classic CNNs such as LeNet-5
and VGGNet. It strikes a balance between performance and clarity. It’s
small enough to train on a CPU, but expressive enough to learn
meaningful features from medical images.</p>
<p>More complex architectures (like DenseNet) are used in real-world
medical imaging applications. But for a small dataset and classroom
setting, our custom architecture is ideal for learning.</p>
<p>To make this process modular and reusable, we’ll write a function
called <code>build_model()</code> using TensorFlow and Keras.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> (</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    Activation, BatchNormalization, Conv2D, Dense,</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    Dropout, GlobalAveragePooling2D, Input, MaxPool2D</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="kw">def</span> build_model(input_shape<span class="op">=</span>(<span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">1</span>), dropout_rate<span class="op">=</span><span class="fl">0.6</span>):</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co">    Build and return a convolutional neural network with explanatory comments.</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a><span class="co">        input_shape (tuple): Shape of the input images (H, W, Channels).</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="co">        dropout_rate (float): Dropout rate to use before final dense layers.</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="co">        model (tf.keras.Model): Compiled Keras model.</span></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>    <span class="co"># Define the input layer matching the shape of the images.</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>    inputs <span class="op">=</span> Input(shape<span class="op">=</span>input_shape)</span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>    <span class="co"># First convolutional layer: applies 8 filters (3x3), followed by max pooling</span></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>    <span class="co"># Padding='same' keeps the output size the same as the input.</span></span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>    x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>    <span class="co"># Add a second convolutional layer + pooling</span></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>    x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>    <span class="co"># Add two more convolutional layers with 12 filters, extracting more complex features</span></span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>    x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>    x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>    <span class="co"># Increase the filter size and depth (20 filters, 5x5 kernel)</span></span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">20</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>    x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">20</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a>    x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a></span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>    <span class="co"># Final convolutional layer with 50 filters</span></span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>    x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">50</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a></span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>    <span class="co"># Global average pooling reduces each feature map to a single value</span></span>
<span id="cb4-46"><a href="#cb4-46" tabindex="-1"></a>    x <span class="op">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb4-47"><a href="#cb4-47" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" tabindex="-1"></a>    <span class="co"># Dense (fully connected) layer with 128 neurons for classification</span></span>
<span id="cb4-49"><a href="#cb4-49" tabindex="-1"></a>    <span class="co"># Dropout applied to the 128 activations from the Dense layer</span></span>
<span id="cb4-50"><a href="#cb4-50" tabindex="-1"></a>    x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-51"><a href="#cb4-51" tabindex="-1"></a>    x <span class="op">=</span> Dropout(dropout_rate)(x)</span>
<span id="cb4-52"><a href="#cb4-52" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" tabindex="-1"></a>    <span class="co"># Another dense layer with 32 neurons</span></span>
<span id="cb4-54"><a href="#cb4-54" tabindex="-1"></a>    x <span class="op">=</span> Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb4-55"><a href="#cb4-55" tabindex="-1"></a></span>
<span id="cb4-56"><a href="#cb4-56" tabindex="-1"></a>    <span class="co"># Final output layer: a single neuron with sigmoid activation (for binary classification)</span></span>
<span id="cb4-57"><a href="#cb4-57" tabindex="-1"></a>    outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb4-58"><a href="#cb4-58" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" tabindex="-1"></a>    <span class="co"># Build the model</span></span>
<span id="cb4-60"><a href="#cb4-60" tabindex="-1"></a>    model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span>
<span id="cb4-61"><a href="#cb4-61" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre>
</div>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>What is the purpose of using multiple convolutional layers in a
neural network?<br>
</li>
<li>What would happen if you skipped the pooling layers entirely?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li><p>Stacking convolutional layers allows the network to learn
increasingly abstract features — early layers detect edges and textures,
while later layers detect shapes or patterns.</p></li>
<li><p>Skipping pooling layers means the model retains high-resolution
spatial information, but it increases computational cost and can lead to
overfitting.</p></li>
</ol>
</div>
</div>
</div>
</div>
<p>Now let’s build the model and view its architecture:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">from</span> tensorflow.random <span class="im">import</span> set_seed</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="co"># Call the build_model function to create the model</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>model <span class="op">=</span> build_model(input_shape<span class="op">=</span>(<span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">1</span>), dropout_rate<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="co"># View the model architecture</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "model_39"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_9 (InputLayer)        [(None, 256, 256, 1)]     0

 conv2d_59 (Conv2D)          (None, 256, 256, 8)       80

 max_pooling2d_50 (MaxPoolin  (None, 128, 128, 8)      0
 g2D)

 conv2d_60 (Conv2D)          (None, 128, 128, 8)       584

 max_pooling2d_51 (MaxPoolin  (None, 64, 64, 8)        0
 g2D)

 conv2d_61 (Conv2D)          (None, 64, 64, 12)        876

 max_pooling2d_52 (MaxPoolin  (None, 32, 32, 12)       0
 g2D)

 conv2d_62 (Conv2D)          (None, 32, 32, 12)        1308

 max_pooling2d_53 (MaxPoolin  (None, 16, 16, 12)       0
 g2D)

 conv2d_63 (Conv2D)          (None, 16, 16, 20)        6020

 max_pooling2d_54 (MaxPoolin  (None, 8, 8, 20)         0
 g2D)

 conv2d_64 (Conv2D)          (None, 8, 8, 20)          10020

 max_pooling2d_55 (MaxPoolin  (None, 4, 4, 20)         0
 g2D)

 conv2d_65 (Conv2D)          (None, 4, 4, 50)          25050

 global_average_pooling2d_8   (None, 50)               0
 (GlobalAveragePooling2D)

 dense_26 (Dense)            (None, 128)               6528

 dropout_8 (Dropout)         (None, 128)               0

 dense_27 (Dense)            (None, 32)                4128

 dense_28 (Dense)            (None, 1)                 33

=================================================================
Total params: 54,627
Trainable params: 54,627
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<div id="exercise-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-1" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<p>Increase the number of filters in the first convolutional layer from
8 to 16.</p>
<ul>
<li>How does this affect the number of parameters in the model?</li>
<li>What effect do you expect this change to have on the model’s
learning capacity?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3"> Show me the solution </h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>In the <code>build_model()</code> function, locate this line:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span></code></pre>
</div>
<p>Change it to:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span></code></pre>
</div>
<p>This increases the number of filters (feature detectors), and
therefore increases the number of learnable parameters. The model may be
able to capture more features, improving learning, but it also risks
overfitting and will take longer to train.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Neural networks are composed of layers of neurons that transform
inputs into outputs through learnable parameters.</li>
<li>Activation functions introduce non-linearity and help neural
networks learn complex patterns.</li>
<li>Dense (fully connected) layers connect every neuron from one layer
to the next and are commonly used in classification tasks.</li>
<li>Convolutional layers apply filters to extract spatial features from
images and are the core of convolutional neural networks (CNNs).</li>
<li>Dropout helps reduce overfitting by randomly disabling neurons
during training.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-05-train_eval"><p>Content from <a href="05-train_eval.html">Training and evaluation</a></p>
<hr>
<p>Last updated on 2025-05-22 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/05-train_eval.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How is a neural network trained to make better predictions?</li>
<li>What do training loss and accuracy tell us?</li>
<li>How do we evaluate a model’s performance on unseen data?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Compile a neural network with a suitable loss function and
optimizer.</li>
<li>Train a convolutional neural network using batches of data.</li>
<li>Monitor model performance during training using training and
validation loss and accuracy.</li>
<li>Evaluate a trained model on a held-out test set.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="compile-and-train-your-model">Compile and train your model<a class="anchor" aria-label="anchor" href="#compile-and-train-your-model"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve defined the architecture of our neural network, the
next step is to compile and train it.</p>
<div class="section level3">
<h3 id="what-does-compiling-a-model-mean">What does “compiling” a model mean?<a class="anchor" aria-label="anchor" href="#what-does-compiling-a-model-mean"></a>
</h3>
<p>Compiling sets up the model for training by specifying:</p>
<ul>
<li>A loss function, which measures the difference between the model’s
predictions and the actual labels.</li>
<li>An optimizer, such as gradient descent, which adjusts the model’s
internal weights to minimize the loss.</li>
<li>One or more metrics, such as accuracy, to evaluate performance
during training.</li>
</ul>
</div>
<div class="section level3">
<h3 id="what-happens-during-training">What happens during training?<a class="anchor" aria-label="anchor" href="#what-happens-during-training"></a>
</h3>
<p>Training is the process of finding the best set of weights to
minimize the loss. This is done by:</p>
<ul>
<li>Making predictions on a batch of training data.</li>
<li>Comparing those predictions to the true labels using the loss
function.</li>
<li>Adjusting the weights to reduce the error, using the optimizer.</li>
</ul>
</div>
<div class="section level3">
<h3 id="what-are-batch-size-steps-per-epoch-and-epochs">What are batch size, steps per epoch, and epochs?<a class="anchor" aria-label="anchor" href="#what-are-batch-size-steps-per-epoch-and-epochs"></a>
</h3>
<ul>
<li>Batch size is the number of training examples processed together
before updating the model’s weights.
<ul>
<li>Smaller batch sizes use less memory and may generalize better but
take longer to train.</li>
<li>Larger batch sizes make faster progress per step but may require
more memory and can sometimes overfit.</li>
</ul>
</li>
<li>Steps per epoch defines how many batches the model processes in one
epoch. A typical setting is:
<code>steps_per_epoch = len(dataset_train) // batch_size</code>
</li>
<li>Epochs refers to how many times the model sees the entire training
dataset.</li>
</ul>
<p>Choosing these parameters is a tradeoff between speed, memory usage,
and performance. You can experiment to find values that work best for
your data and hardware.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> optimizers</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.callbacks <span class="im">import</span> ModelCheckpoint</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Define the network optimization method. </span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># Adam is a popular gradient descent algorithm</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># with adaptive, per-parameter learning rates.</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>custom_adam <span class="op">=</span> optimizers.Adam()</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Compile the model defining the 'loss' function type, optimization and the metric.</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, optimizer<span class="op">=</span>custom_adam, metrics<span class="op">=</span>[<span class="st">'acc'</span>])</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Save the best model found during training</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>checkpointer <span class="op">=</span> ModelCheckpoint(filepath<span class="op">=</span><span class="st">'best_model.keras'</span>, monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>                               verbose<span class="op">=</span><span class="dv">1</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co"># Training parameters</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co"># Start the timer</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="co"># Now train our network!</span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="co"># steps_per_epoch = len(dataset_train)//batch_size</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>hist <span class="op">=</span> model.fit(datagen.flow(dataset_train, labels_train, batch_size<span class="op">=</span>batch_size),</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>                 epochs<span class="op">=</span>epochs, </span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>                 validation_data<span class="op">=</span>(dataset_val, labels_val), </span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>                 callbacks<span class="op">=</span>[checkpointer])</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a><span class="co"># End the timer</span></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>elapsed_time <span class="op">=</span> end_time <span class="op">-</span> start_time</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training completed in </span><span class="sc">{</span>elapsed_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span></code></pre>
</div>
<p>We can now plot the results of the training. “Loss” should drop over
successive epochs and accuracy should increase.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'loss'</span>], <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'train loss'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'val_loss'</span>], <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'val loss'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>plt.show()</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'acc'</span>], <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'train accuracy'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'val_acc'</span>], <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'val accuracy'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="fig/training_curves.png" alt="Training curves" width="600" class="figure mx-auto d-block"></figure><div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<p>Examine the training and validation curves.</p>
<ol style="list-style-type: upper-alpha">
<li>What does it mean if the training loss continues to decrease, but
the validation loss starts increasing?<br>
</li>
<li>Suggest two actions you could take to reduce overfitting in this
situation.<br>
</li>
<li>Bonus: Try increasing the dropout rate in your model. What happens
to the validation accuracy?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li><p>If the training loss decreases while the validation loss
increases, the model is <strong>overfitting</strong> — it’s learning the
training data too well and struggling to generalize to unseen
data.</p></li>
<li><p>You could:</p></li>
</ol>
<ul>
<li>
<strong>Increase regularization</strong> (e.g. by raising the
dropout rate)</li>
<li><strong>Add more training data</strong></li>
<li><strong>Use data augmentation</strong></li>
<li>
<strong>Simplify the model</strong> to reduce capacity</li>
</ul>
<ol start="3" style="list-style-type: upper-alpha">
<li>Increasing dropout may lower performance slightly but improve
generalization. Always compare the training and validation accuracy/loss
to decide.</li>
</ol>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="batch-normalization">Batch normalization<a class="anchor" aria-label="anchor" href="#batch-normalization"></a>
</h2>
<hr class="half-width">
<p><a href="https://keras.io/api/layers/normalization_layers/batch_normalization/" class="external-link">Batch
normalization</a> is a technique that standardizes the output of a layer
across each training batch. This helps stabilize and speed up
training.</p>
<p>It works by:</p>
<ul>
<li>Subtracting the batch mean</li>
<li>Dividing by the batch standard deviation</li>
<li>Applying a learnable scale and shift</li>
</ul>
<p>You typically insert <code>BatchNormalization()</code> after a
convolutional or dense layer, and before the activation function:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>x <span class="op">=</span> Conv2D(<span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>)(x)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>x <span class="op">=</span> BatchNormalization()(x)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>x <span class="op">=</span> Activation(<span class="st">'relu'</span>)(x)</span></code></pre>
</div>
<p>Benefits can include:</p>
<ul>
<li>Faster training</li>
<li>Reduced sensitivity to weight initialization</li>
<li>Helps prevent overfitting</li>
</ul>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Try inserting a BatchNormalization() layer after the first
convolutional layer in your model, and re-run the training.
Compare:</li>
</ol>
<ul>
<li>Training time</li>
<li>Accuracy</li>
<li>Validation performance</li>
</ul>
<p>What changes do you notice?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>Adding batch normalization can improve training stability and
accuracy. Find this line in your model:</li>
</ol>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span></code></pre>
</div>
<p>Split it into two lines, and insert <code>BatchNormalization()</code>
before the activation:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>)(inputs)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>x <span class="op">=</span> BatchNormalization()(x)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>x <span class="op">=</span> Activation(<span class="st">'relu'</span>)(x)</span></code></pre>
</div>
<p>You may notice:</p>
<ul>
<li>Smoother training curves</li>
<li>Higher validation accuracy</li>
<li>Slightly faster convergence</li>
</ul>
<p>Remember to retrain your model after making this change.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="choosing-and-modifying-the-architecture">Choosing and modifying the architecture<a class="anchor" aria-label="anchor" href="#choosing-and-modifying-the-architecture"></a>
</h2>
<hr class="half-width">
<p>There is no single “correct” architecture for a neural network. The
best design depends on your data, task, and computational constraints.
Here is a systematic approach to designing and improving your model
architecture:</p>
<div class="section level3">
<h3 id="start-simple">Start simple<a class="anchor" aria-label="anchor" href="#start-simple"></a>
</h3>
<p>Begin with a basic model and verify that it can learn from your data.
It is better to get a simple model working than to over-complicate
things early.</p>
</div>
<div class="section level3">
<h3 id="use-proven-patterns">Use proven patterns<a class="anchor" aria-label="anchor" href="#use-proven-patterns"></a>
</h3>
<p>Borrow ideas from successful models:</p>
<ul>
<li>LeNet-5: good for small grayscale images.</li>
<li>VGG: uses repeated 3×3 convolutions and pooling.</li>
<li>ResNet or DenseNet: useful for deep networks with skip
connections.</li>
</ul>
</div>
<div class="section level3">
<h3 id="tune-hyperparameters-systematically">Tune hyperparameters systematically<a class="anchor" aria-label="anchor" href="#tune-hyperparameters-systematically"></a>
</h3>
<p>To improve performance in a structured way, try:</p>
<ul>
<li>Manual tuning: Change one variable at a time (e.g., number of
filters, dropout rate) and observe its effect on validation
performance.</li>
<li>Grid search: Define a grid of parameters (e.g., filter sizes,
learning rates, dropout values) and test all combinations. This is slow
but thorough.</li>
<li>Automated tuning: Use tools like <a href="https://keras.io/keras_tuner/" class="external-link">Keras Tuner</a> to automate the
search for the best architecture.</li>
</ul>
</div>
<div class="section level3">
<h3 id="evaluate-and-iterate">Evaluate and iterate<a class="anchor" aria-label="anchor" href="#evaluate-and-iterate"></a>
</h3>
<p>Use validation performance to guide decisions:</p>
<ul>
<li>Does adding filters or layers improve accuracy?</li>
<li>Is the model overfitting (training accuracy much higher than
validation)?</li>
<li>Is training time manageable?</li>
</ul>
</div>
<div class="section level3">
<h3 id="use-regularization">Use regularization<a class="anchor" aria-label="anchor" href="#use-regularization"></a>
</h3>
<p>To reduce overfitting, consider:</p>
<ul>
<li>Dropout layers</li>
<li>Data augmentation</li>
<li>Early stopping</li>
</ul>
</div>
</section><section><h2 class="section-heading" id="evaluating-your-model-on-the-held-out-test-set">Evaluating your model on the held-out test set<a class="anchor" aria-label="anchor" href="#evaluating-your-model-on-the-held-out-test-set"></a>
</h2>
<hr class="half-width">
<p>In this step, we present the unseen test dataset to our trained
network and evaluate the performance.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> load_model </span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="co"># Open the best model saved during training</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>best_model <span class="op">=</span> load_model(<span class="st">'best_model.keras'</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Neural network weights updated to the best epoch.'</span>)</span></code></pre>
</div>
<p>Now that we’ve loaded the best model, we can evaluate the accuracy on
our test data.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># We use the evaluate function to evaluate the accuracy of our model in the test group</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy in test group: </span><span class="sc">{</span>best_model<span class="sc">.</span>evaluate(dataset_test, labels_test, verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Accuracy in test group: 0.80</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Neural networks are trained by adjusting weights to minimize a loss
function using optimization algorithms like Adam.</li>
<li>Training is done in batches over multiple epochs to gradually
improve performance.</li>
<li>Validation data helps detect overfitting and track generalization
during training.</li>
<li>The best model can be selected by monitoring validation loss and
saved for future use.</li>
<li>Final performance should be evaluated on a separate test set that
the model has not seen during training.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-06-explainability"><p>Content from <a href="06-explainability.html">Explainability</a></p>
<hr>
<p>Last updated on 2025-05-21 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/06-explainability.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a saliency map, and how is it used to explain model
predictions?</li>
<li>How do different explainability methods (e.g., GradCAM++
vs. ScoreCAM) compare?</li>
<li>What are the limitations of saliency maps in practice?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Understand how saliency maps highlight regions that influence model
predictions.</li>
<li>Generate saliency maps using GradCAM++ and ScoreCAM.</li>
<li>Compare explainability methods and assess their reliability.</li>
<li>Reflect on the strengths and limitations of visual explanation
techniques.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="explainability">Explainability<a class="anchor" aria-label="anchor" href="#explainability"></a>
</h2>
<hr class="half-width">
<p>If a model is making a prediction, many of us would like to know how
the decision was reached. Saliency maps - and related approaches - are a
popular form of explainability for imaging models.</p>
<p>Saliency maps use color to illustrate the extent to which a region of
an image contributes to a given decision. Let’s plot some saliency maps
for our model:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># !pip install tf_keras_vis</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.gradcam <span class="im">import</span> Gradcam</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.gradcam_plus_plus <span class="im">import</span> GradcamPlusPlus</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.scorecam <span class="im">import</span> Scorecam</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.utils.scores <span class="im">import</span> CategoricalScore</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># Select two differing explainability algorithms</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>gradcam <span class="op">=</span> GradcamPlusPlus(best_model, clone<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>scorecam <span class="op">=</span> Scorecam(best_model, clone<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="kw">def</span> plot_map(cam, classe, prediction, img):</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co">    Plot the image.</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>    axes[<span class="dv">0</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    axes[<span class="dv">1</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    heatmap <span class="op">=</span> np.uint8(cm.jet(cam[<span class="dv">0</span>])[..., :<span class="dv">3</span>] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>    i <span class="op">=</span> axes[<span class="dv">1</span>].imshow(heatmap, cmap<span class="op">=</span><span class="st">"jet"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    fig.colorbar(i)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>    plt.suptitle(<span class="st">"Class: </span><span class="sc">{}</span><span class="st">. Pred = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(classe, prediction))</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># Plot each image with accompanying saliency map</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="cf">for</span> image_id <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>    SEED_INPUT <span class="op">=</span> dataset_test[image_id]</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>    CATEGORICAL_INDEX <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>    layer_idx <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>    penultimate_layer_idx <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>    class_idx  <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>    cat_score <span class="op">=</span> labels_test[image_id]</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>    cat_score <span class="op">=</span> CategoricalScore(CATEGORICAL_INDEX)</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>    cam <span class="op">=</span> gradcam(cat_score, SEED_INPUT, </span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>                  penultimate_layer <span class="op">=</span> penultimate_layer_idx,</span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>                  normalize_cam<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a>    <span class="co"># Display the class</span></span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a>    _class <span class="op">=</span> <span class="st">'normal'</span> <span class="cf">if</span> labels_test[image_id] <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'effusion'</span></span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a>    _prediction <span class="op">=</span> best_model.predict(dataset_test[image_id][np.newaxis, :, ...], verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a>    </span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a>    plot_map(cam, _class, _prediction[<span class="dv">0</span>][<span class="dv">0</span>], SEED_INPUT)</span></code></pre>
</div>
<figure><img src="fig/saliency.png" alt="Saliency maps" width="600" class="figure mx-auto d-block"></figure><div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Choose three saliency maps from your outputs and describe:</li>
</ol>
<ul>
<li>Where the model focused its attention</li>
<li>Whether this attention seems clinically meaningful</li>
<li>Any surprising or questionable results</li>
</ul>
<p>Discuss with a partner: does the model seem to be making decisions
for the right reasons?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>You may find that some maps highlight areas around the lungs,
suggesting the model is learning useful clinical features. Other maps
might focus on irrelevant regions (e.g., borders or artifacts), which
could suggest model overfitting or dataset biases.</li>
</ol>
<p>Interpreting these results requires domain knowledge and critical
thinking. This exercise is designed to foster discussion rather than
provide a single right answer.</p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="sanity-checks-for-saliency-maps">Sanity checks for saliency maps<a class="anchor" aria-label="anchor" href="#sanity-checks-for-saliency-maps"></a>
</h2>
<hr class="half-width">
<p>While saliency maps may offer us interesting insights about regions
of an image contributing to a model’s output, there are suggestions that
this kind of visual assessment can be misleading. For example, the
following abstract is from a paper entitled “<a href="https://arxiv.org/abs/1810.03292" class="external-link">Sanity Checks for Saliency
Maps</a>”:</p>
<blockquote>
<p>Saliency methods have emerged as a popular tool to highlight features
in an input deemed relevant for the prediction of a learned model.
Several saliency methods have been proposed, often guided by visual
appeal on image data. … Through extensive experiments we show that some
existing saliency methods are independent both of the model and of the
data generating process. Consequently, methods that fail the proposed
tests are inadequate for tasks that are sensitive to either data or
model, such as, finding outliers in the data, explaining the
relationship between inputs and outputs that the model learned, and
debugging the model.</p>
</blockquote>
<p>There are multiple methods for producing saliency maps to explain how
a particular model is making predictions. The method we have been using
is called GradCam++, but how does this method compare to another? Use
this code to compare GradCam++ with ScoreCam.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">def</span> plot_map2(cam1, cam2, classe, prediction, img):</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co">    Plot the image.</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    axes[<span class="dv">0</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    axes[<span class="dv">1</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    axes[<span class="dv">2</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>    heatmap1 <span class="op">=</span> np.uint8(cm.jet(cam1[<span class="dv">0</span>])[..., :<span class="dv">3</span>] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    heatmap2 <span class="op">=</span> np.uint8(cm.jet(cam2[<span class="dv">0</span>])[..., :<span class="dv">3</span>] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    i <span class="op">=</span> axes[<span class="dv">1</span>].imshow(heatmap1, cmap<span class="op">=</span><span class="st">"jet"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>    j <span class="op">=</span> axes[<span class="dv">2</span>].imshow(heatmap2, cmap<span class="op">=</span><span class="st">"jet"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>    fig.colorbar(i)</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    plt.suptitle(<span class="st">"Class: </span><span class="sc">{}</span><span class="st">. Pred = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(classe, prediction))</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co"># Plot each image with accompanying saliency map</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="cf">for</span> image_id <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>    SEED_INPUT <span class="op">=</span> dataset_test[image_id]</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>    CATEGORICAL_INDEX <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>    layer_idx <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>    penultimate_layer_idx <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>    class_idx  <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>    cat_score <span class="op">=</span> labels_test[image_id]</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>    cat_score <span class="op">=</span> CategoricalScore(CATEGORICAL_INDEX)</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>    cam <span class="op">=</span> gradcam(cat_score, SEED_INPUT, </span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>                  penultimate_layer <span class="op">=</span> penultimate_layer_idx,</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>                  normalize_cam<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>    cam2 <span class="op">=</span> scorecam(cat_score, SEED_INPUT, </span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a>                  penultimate_layer <span class="op">=</span> penultimate_layer_idx,</span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a>                  normalize_cam<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a>                  )</span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a>    </span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a>    <span class="co"># Display the class</span></span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a>    _class <span class="op">=</span> <span class="st">'normal'</span> <span class="cf">if</span> labels_test[image_id] <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'effusion'</span></span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a>    _prediction <span class="op">=</span> best_model.predict(dataset_test[image_id][np.newaxis, : ,...], verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>    </span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    plot_map2(cam, cam2, _class, _prediction[<span class="dv">0</span>][<span class="dv">0</span>], SEED_INPUT)</span></code></pre>
</div>
<p>Some of the time these methods largely agree:</p>
<figure><img src="fig/saliency-agreement.png" alt="saliency_agreement" class="figure mx-auto d-block"></figure><p>But some of the time they disagree wildly:</p>
<figure><img src="fig/saliency-disagreement.png" alt="saliency_disagreement" class="figure mx-auto d-block"></figure><p>This raises the question, should these algorithms be used at all?</p>
<p>This is part of a larger problem with explainability of complex
models in machine learning. The generally accepted answer is to know
<strong>how your model works</strong> and to know <strong>how your
explainability algorithm works</strong> as well as to <strong>understand
your data</strong>.</p>
<p>With these three pieces of knowledge it should be possible to
identify algorithms appropriate for your task, and to understand any
shortcomings in their approaches.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Saliency maps visualize which parts of an image contribute most to a
model’s prediction.</li>
<li>GradCAM++ and ScoreCAM are commonly used techniques for generating
saliency maps in convolutional models.</li>
<li>Saliency maps can help build trust in a model, but they may not
always reflect true model behavior.</li>
<li>Explainability methods should be interpreted cautiously and
validated carefully.</li>
</ul>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/machine-learning-neural-python/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:tpollard@mit.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.12" class="external-link">sandpaper (0.16.12)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.6" class="external-link">varnish (1.0.6)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries-incubator.github.io/machine-learning-neural-python/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/machine-learning-neural-python/aio.html",
  "identifier": "https://carpentries-incubator.github.io/machine-learning-neural-python/aio.html",
  "dateCreated": "2021-10-25",
  "dateModified": "2025-05-22",
  "datePublished": "2025-05-22"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

