<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to artificial neural networks in Python: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="../favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Lesson Description" src="../assets/images/incubator-logo.svg"><span class="badge text-bg-info">
          <abbr title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
              Beta
            </a>
            <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to artificial neural networks in Python
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to artificial neural networks in Python
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to artificial neural networks in Python
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-xrays.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-visualisation.html">2. Visualisation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-data_prep.html">3. Data preparation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-create_net.html">4. Neural networks</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="05-train_eval.html">5. Training and evaluation</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-explainability.html">6. Explainability</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-xrays"><p>Content from <a href="01-xrays.html">Introduction</a></p>
<hr>
<p>Last updated on 2025-05-05 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/01-xrays.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What kinds of diseases can be observed in chest X-rays?</li>
<li>What is pleural effusion?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Gain awareness of the NIH ChestX-ray dataset.</li>
<li>Load a subset of labelled chest X-rays.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="chest-x-rays">Chest X-rays<a class="anchor" aria-label="anchor" href="#chest-x-rays"></a>
</h2>
<hr class="half-width">
<p>Chest X-rays are frequently used in healthcare to view the heart,
lungs, and bones of patients. On an X-ray, broadly speaking, bones
appear white, soft tissue appears grey, and air appears black. The
images can show details such as:</p>
<ul>
<li>Lung conditions, for example pneumonia, emphysema, or air in the
space around the lung.</li>
<li>Heart conditions, such as heart failure or heart valve
problems.</li>
<li>Bone conditions, such as rib or spine fractures</li>
<li>Medical devices, such as pacemaker, defibrillators and catheters.
X-rays are often taken to assess whether these devices are positioned
correctly.</li>
</ul>
<p>In recent years, organisations like the <a href="https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community" class="external-link">National
Institutes of Health</a> have released large collections of X-rays,
labelled with common diseases. The goal is to stimulate the community to
develop algorithms that might assist radiologists in making diagnoses,
and to potentially discover other findings that may have been
overlooked.</p>
<p>The following figure is from a study by <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf" class="external-link">Xiaosong
Wang et al</a>. It illustrates eight common diseases that the authors
noted could be be detected and even spatially-located in front chest
x-rays with the use of modern machine learning algorithms.</p>
<figure><img src="../fig/wang_et_al.png" alt="Chest X-ray diseases" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="pleural-effusion">Pleural effusion<a class="anchor" aria-label="anchor" href="#pleural-effusion"></a>
</h2>
<hr class="half-width">
<p>Thin membranes called “pleura” line the lungs and facilitate
breathing. Normally there is a small amount of fluid present in the
pleura, but certain conditions can cause excess build-up of fluid. This
build-up is known as pleural effusion, sometimes referred to as “water
on the lungs”.</p>
<p>Causes of pleural effusion vary widely, ranging from mild viral
infections to serious conditions such as congestive heart failure and
cancer. In an upright patient, fluid gathers in the lowest part of the
chest, and this build up is visible to an expert.</p>
<p>For the remainder of this lesson, we will develop an algorithm to
detect pleural effusion in chest X-rays. Specifically, using a set of
chest X-rays labelled as either “normal” or “pleural effusion”, we will
train a neural network to classify unseen chest X-rays into one of these
classes.</p>
</section><section><h2 class="section-heading" id="loading-the-dataset">Loading the dataset<a class="anchor" aria-label="anchor" href="#loading-the-dataset"></a>
</h2>
<hr class="half-width">
<p>The data that we are going to use for this project consists of 350
“normal” chest X-rays and 350 X-rays that are labelled as showing
evidence pleural effusion. These X-rays are a subset of the public NIH
ChestX-ray dataset.</p>
<blockquote>
<p>Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri,
Ronald Summers, ChestX-ray8: Hospital-scale Chest X-ray Database and
Benchmarks on Weakly-Supervised Classification and Localization of
Common Thorax Diseases, IEEE CVPR, pp. 3462-3471, 2017</p>
</blockquote>
<p>Let’s begin by loading the dataset.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># The glob module finds all the pathnames matching a specified pattern</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> glob <span class="im">import</span> glob</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># If your dataset is compressed, unzip with:</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># !unzip chest_xrays.zip</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># Define folders containing images</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>data_path <span class="op">=</span> os.path.join(<span class="st">"chest_xrays"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>effusion_path <span class="op">=</span> os.path.join(data_path, <span class="st">"effusion"</span>, <span class="st">"*.png"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>normal_path <span class="op">=</span> os.path.join(data_path, <span class="st">"normal"</span>, <span class="st">"*.png"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Create list of files</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>effusion_list <span class="op">=</span> glob(effusion_path)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>normal_list <span class="op">=</span> glob(normal_path)</span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of cases with pleural effusion: '</span>, <span class="bu">len</span>(effusion_list)) </span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of normal cases: '</span>, <span class="bu">len</span>(normal_list))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of cases with pleural effusion:  350
Number of normal cases:  350</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Algorithms can be used to detect disease in chest X-rays.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-02-visualisation"><p>Content from <a href="02-visualisation.html">Visualisation</a></p>
<hr>
<p>Last updated on 2025-05-05 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/02-visualisation.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does an image with pleural effusion differ from one
without?</li>
<li>How is image data represented in a NumPy array?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Visually compare normal X-rays with those labelled with pleural
effusion.</li>
<li>Understand how to use NumPy to store and manipulate image data.</li>
<li>Compare a slice of numerical data to its corresponding image.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="visualising-the-x-rays">Visualising the X-rays<a class="anchor" aria-label="anchor" href="#visualising-the-x-rays"></a>
</h2>
<hr class="half-width">
<p>In the previous section, we set up a dataset comprising 700 chest
X-rays. Half of the X-rays are labelled “normal” and half are labelled
as “pleural effusion”. Let’s take a look at some of the images.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># cv2 is openCV, a popular computer vision library</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt </span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="kw">def</span> plot_example(example, label, loc):</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    image <span class="op">=</span> cv2.imread(example)</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    im <span class="op">=</span> ax[loc].imshow(image)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    title <span class="op">=</span> <span class="ss">f"Class: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>example<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>    ax[loc].set_title(title)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>fig.set_size_inches(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Plot a "normal" record</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>plot_example(random.choice(normal_list), <span class="st">"Normal"</span>, <span class="dv">0</span>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co"># Plot a record labelled with effusion</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>plot_example(random.choice(effusion_list), <span class="st">"Effusion"</span>, <span class="dv">1</span>)</span></code></pre>
</div>
<figure><img src="../fig/example_records.png" alt="Example X-rays" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="can-we-detect-effusion">Can we detect effusion?<a class="anchor" aria-label="anchor" href="#can-we-detect-effusion"></a>
</h2>
<hr class="half-width">
<p>Run the following code to flip a coin to select an x-ray from our
collection.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Effusion or not?"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># flip a coin</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>coin_flip <span class="op">=</span> random.choice([<span class="st">"Effusion"</span>, <span class="st">"Normal"</span>])</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="cf">if</span> coin_flip <span class="op">==</span> <span class="st">"Normal"</span>:</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    fn <span class="op">=</span> random.choice(normal_list)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    fn <span class="op">=</span> random.choice(effusion_list)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># plot the image</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(fn)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>plt.imshow(image)</span></code></pre>
</div>
<p>Show the answer:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Jupyter doesn't allow us to print the image until the cell has run,</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co"># so we'll print in a new cell.</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The answer is: </span><span class="sc">{</span>coin_flip<span class="sc">}</span><span class="ss">!"</span>)</span></code></pre>
</div>
<div id="exercise" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise" class="callout-inner">
<h3 class="callout-title">Exercise</h3>
<div class="callout-content">
<ol style="list-style-type: upper-alpha">
<li>Manually classify 10 X-rays using the coin flip code. Make a note of
your predictive accuracy (hint: for a reminder of the formula for
accuracy, check the solution below).</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ol style="list-style-type: upper-alpha">
<li>Accuracy is the fraction of predictions that were correct (correct
predictions / total predictions). If you made 10 predictions and 5 were
correct, your accuracy is 50%.</li>
</ol>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="how-does-a-computer-see-an-image">How does a computer see an image?<a class="anchor" aria-label="anchor" href="#how-does-a-computer-see-an-image"></a>
</h2>
<hr class="half-width">
<p>Consider an image as a matrix in which the value of each pixel
corresponds to a number that determines a tone or color. Let’s load one
of our images:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>file_idx <span class="op">=</span> <span class="dv">56</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>example <span class="op">=</span> normal_list[file_idx]</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(example)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(512, 512, 3)</code></pre>
</div>
<p>Here we see that the image has 3 dimensions. The first dimension is
height (512 pixels) and the second is width (also 512 pixels). The
presence of a third dimension indicates that we are looking at a color
image (“RGB”, or Red, Green, Blue).</p>
<p>For more detail on image representation in Python, take a look at the
<a href="https://datacarpentry.org/image-processing/" class="external-link">Data Carpentry
course on Image Processing with Python</a>. The following image is
reproduced from the <a href="https://datacarpentry.org/image-processing/03-skimage-images/index.html" class="external-link">section
on Image Representation</a>.</p>
<figure><img src="../fig/chair-layers-rgb.png" alt="RGB image" width="600" class="figure mx-auto d-block"></figure><p>For simplicity, we’ll instead load the images in greyscale. A
greyscale image has two dimensions: height and width. Greyscale images
have only one channel. Most greyscale images are 8 bits per channel or
16 bits per channel. For a greyscale image with 8 bits per channel, each
value in the matrix represents a tone between black (0) and white
(255).</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(example, cv2.IMREAD_GRAYSCALE)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(512, 512)</code></pre>
</div>
<p>Let’s briefly display the matrix of values, and then see how these
same values are rendered as an image.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="co"># Print a 10 by 10 chunk of the matrix</span></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="bu">print</span>(image[<span class="dv">35</span>:<span class="dv">45</span>, <span class="dv">30</span>:<span class="dv">40</span>])</span></code></pre>
</div>
<figure><img src="../fig/greyscale_example_numpy.png" alt="Example greyscale numpy array" width="400" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># Plot the same chunk as an image</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>plt.imshow(image[<span class="dv">35</span>:<span class="dv">45</span>, <span class="dv">30</span>:<span class="dv">40</span>], cmap<span class="op">=</span><span class="st">'gray'</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">255</span>)</span></code></pre>
</div>
<figure><img src="../fig/greyscale_example.png" alt="Example greyscale image" width="400" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="image-pre-processing">Image pre-processing<a class="anchor" aria-label="anchor" href="#image-pre-processing"></a>
</h2>
<hr class="half-width">
<p>In the next episode, we’ll be building and training a model. Let’s
prepare our data for the modelling phase. For convenience, we’ll begin
by loading all of the images and corresponding labels and assigning them
to a list.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># create a list of effusion images and labels</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>dataset_effusion <span class="op">=</span> [cv2.imread(fn, cv2.IMREAD_GRAYSCALE) <span class="cf">for</span> fn <span class="kw">in</span> effusion_list]</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>label_effusion <span class="op">=</span> np.ones(<span class="bu">len</span>(dataset_effusion))</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co"># create a list of normal images and labels</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>dataset_normal <span class="op">=</span> [cv2.imread(fn, cv2.IMREAD_GRAYSCALE) <span class="cf">for</span> fn <span class="kw">in</span> normal_list]</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>label_normal <span class="op">=</span> np.zeros(<span class="bu">len</span>(dataset_normal))</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co"># Combine the lists</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>dataset <span class="op">=</span> dataset_effusion <span class="op">+</span> dataset_normal</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>labels <span class="op">=</span> np.concatenate([label_effusion, label_normal])</span></code></pre>
</div>
<p>Let’s also downsample the images, reducing the size from (512, 512)
to (256,256).</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="co"># Downsample the images from (512,512) to (256,256)</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>dataset <span class="op">=</span> [cv2.resize(img, (<span class="dv">256</span>,<span class="dv">256</span>)) <span class="cf">for</span> img <span class="kw">in</span> dataset]</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># Check the size of the reshaped images</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="bu">print</span>(dataset[<span class="dv">0</span>].shape)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="co"># Normalize the data</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a><span class="co"># Subtract the mean, divide by the standard deviation.</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dataset)):</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>  dataset[i] <span class="op">=</span> (dataset[i] <span class="op">-</span> np.average(dataset[i], axis<span class="op">=</span> (<span class="dv">0</span>, <span class="dv">1</span>))) <span class="op">/</span> np.std(dataset[i], axis<span class="op">=</span> (<span class="dv">0</span>, <span class="dv">1</span>)) </span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(256, 256)</code></pre>
</div>
<p>Finally, we’ll convert our dataset from a list to an array. We are
expecting it to be (700, 256, 256). That is 700 images (350 effusion
cases and 350 normal), each with a dimension of 256 by 256.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>dataset <span class="op">=</span> np.asarray(dataset, dtype<span class="op">=</span>np.float32)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Matrix Dimensions: </span><span class="sc">{</span>dataset<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(700, 256, 256)</code></pre>
</div>
<p>We could plot the images by indexing them on <code>dataset</code>,
e.g., we can plot the first image in the dataset with:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>vals <span class="op">=</span> dataset[idx].flatten()</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>plt.imshow(dataset[idx], cmap<span class="op">=</span><span class="st">'gray'</span>, vmin<span class="op">=</span><span class="bu">min</span>(vals), vmax<span class="op">=</span><span class="bu">max</span>(vals))</span></code></pre>
</div>
<figure><img src="../fig/final_example_image.png" alt="Example greyscale image" width="400" class="figure mx-auto d-block"></figure><div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>In NumPy, RGB images are usually stored as 3-dimensional
arrays.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-03-data_prep"><p>Content from <a href="03-data_prep.html">Data preparation</a></p>
<hr>
<p>Last updated on 2025-05-05 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/03-data_prep.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is the purpose of data augmentation?</li>
<li>What types of transform can be applied in data augmentation?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Generate an augmented dataset</li>
<li>Partition data into training and test sets.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="partitioning-into-training-and-test-sets">Partitioning into training and test sets<a class="anchor" aria-label="anchor" href="#partitioning-into-training-and-test-sets"></a>
</h2>
<hr class="half-width">
<p>As we have done in previous projects, we will want to split our data
into subsets for training and testing. The training set is used for
building our model and our test set is used for evaluation.</p>
<p>To ensure reproducibility, we should set the random state of the
splitting method. This means that Python’s random number generator will
produce the same “random” split in future.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># Our Tensorflow model requires the input to be:</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># [batch, height, width, n_channels]</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># So we need to add a dimension to the dataset and labels.</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># Ellipsis (...) is shorthand for selecting with ":" across dimensions. </span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># np.newaxis expands the selection by one dimension.</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>dataset <span class="op">=</span> dataset[..., np.newaxis]</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>labels <span class="op">=</span> labels[..., np.newaxis]</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Create training and test sets</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>dataset_train, dataset_test, labels_train, labels_test <span class="op">=</span> train_test_split(dataset, labels, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="co"># Create a validation set</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>dataset_train, dataset_val, labels_train, labels_val <span class="op">=</span> train_test_split(dataset_train, labels_train, test_size<span class="op">=</span><span class="fl">0.15</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"No. images, x_dim, y_dim, colors) (No. labels, 1)</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Train: </span><span class="sc">{</span>dataset_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>labels_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Validation: </span><span class="sc">{</span>dataset_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>labels_val<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test: </span><span class="sc">{</span>dataset_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>labels_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>No. images, x_dim, y_dim, colors) (No. labels, 1)

Train: (505, 256, 256, 1), (505, 1)
Validation: (90, 256, 256, 1), (90, 1)
Test: (105, 256, 256, 1), (105, 1)</code></pre>
</div>
</section><section><h2 class="section-heading" id="data-augmentation">Data Augmentation<a class="anchor" aria-label="anchor" href="#data-augmentation"></a>
</h2>
<hr class="half-width">
<p>We have a small dataset, which increases the chance of overfitting
our model. If our model is overfitted, it becomes less able to
generalize to data outside the training data.</p>
<p>To artificially increase the size of our training set, we can use
<code>ImageDataGenerator</code>. This function generates new data by
applying random transformations to our source images while our model is
training.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.image <span class="im">import</span> ImageDataGenerator</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># Define what kind of transformations we would like to apply</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co"># such as rotation, crop, zoom, position shift, etc</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>datagen <span class="op">=</span> ImageDataGenerator(</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>    rotation_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>    width_shift_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>    height_shift_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>    zoom_range<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>    horizontal_flip<span class="op">=</span><span class="va">False</span>)</span></code></pre>
</div>
<p>For the sake of interest, let’s take a look at some examples of the
augmented images!</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># specify path to source data</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>path <span class="op">=</span> os.path.join(<span class="st">"chest_xrays"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>batch_size<span class="op">=</span><span class="dv">5</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>val_generator <span class="op">=</span> datagen.flow_from_directory(</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>        path, color_mode<span class="op">=</span><span class="st">"rgb"</span>,</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        target_size<span class="op">=</span>(<span class="dv">256</span>, <span class="dv">256</span>),</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        batch_size<span class="op">=</span>batch_size)</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="kw">def</span> plot_images(images_arr):</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">5</span>, figsize<span class="op">=</span>(<span class="dv">20</span>,<span class="dv">20</span>))</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    axes <span class="op">=</span> axes.flatten()</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>    <span class="cf">for</span> img, ax <span class="kw">in</span> <span class="bu">zip</span>(images_arr, axes):</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>        ax.imshow(img.astype(<span class="st">'uint8'</span>))</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>    plt.show()</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a>augmented_images <span class="op">=</span> [val_generator[<span class="dv">0</span>][<span class="dv">0</span>][<span class="dv">0</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size)]</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>plot_images(augmented_images)</span></code></pre>
</div>
<figure><img src="../fig/xray_augmented.png" alt="X-ray augmented" width="1200" class="figure mx-auto d-block"></figure><p>The images look a little strange, but that’s the idea! When our model
sees something unusual in real-life, it will be better adapted to deal
with it.</p>
<p>Now we have some data to work with, let’s start building our
model.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Data augmentation can help to avoid overfitting.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-04-create_net"><p>Content from <a href="04-create_net.html">Neural networks</a></p>
<hr>
<p>Last updated on 2025-05-05 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/04-create_net.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a neural network?</li>
<li>What are the characteristics of a dense layer?</li>
<li>What is an activation function?</li>
<li>What is a convolutional neural network?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Become familiar with key components of a neural network.</li>
<li>Create the architecture for a convolutational neural network.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="what-is-a-neural-network">What is a neural network?<a class="anchor" aria-label="anchor" href="#what-is-a-neural-network"></a>
</h2>
<hr class="half-width">
<p>An artificial neural network, or just “neural network”, is a broad
term that describes a family of machine learning models that are (very!)
loosely based on the neural circuits found in biology.</p>
<p>The smallest building block of a neural network is a single neuron. A
typical neuron receives inputs (x1, x2, x3) which are multiplied by
learnable weights (w1, w2, w3), then summed with a bias term (b). An
activation function (f) determines the neuron output.</p>
<figure><img src="../fig/neuron.png" alt="Neuron" width="600" class="figure mx-auto d-block"></figure><p>From a high level, a neural network is a system that takes input
values in an “input layer”, processes these values with a collection of
functions in one or more “hidden layers”, and then generates an output
such as a prediction. The network has parameters that are systematically
tweaked to allow pattern recognition.</p>
<figure><img src="../fig/simple_neural_network.png" alt="Neuron" width="800" class="figure mx-auto d-block"></figure><p>The layers shown in the network above are “dense” or “fully
connected”. Each neuron is connected to all neurons in the preceeding
layer. Dense layers are a common building block in neural network
architectures.</p>
<p>“Deep learning” is an increasingly popular term used to describe
certain types of neural network. When people talk about deep learning
they are typically referring to more complex network designs, often with
a large number of hidden layers.</p>
</section><section><h2 class="section-heading" id="activation-functions">Activation Functions<a class="anchor" aria-label="anchor" href="#activation-functions"></a>
</h2>
<hr class="half-width">
<p>Part of the concept of a neural network is that each neuron can
either be ‘active’ or ‘inactive’. This notion of activity and inactivity
is attempted to be replicated by so called activation functions. The
original activation function was the sigmoid function (related to its
use in logistic regression). This would make each neuron’s activation
some number between 0 and 1, with the idea that 0 was ‘inactive’ and 1
was ‘active’.</p>
<p>As time went on, different activation functions were used. For
example the tanh function (hyperbolic tangent function), where the idea
is a neuron can be active in both a positive capacity (close to 1), a
negative capacity (close to -1) or can be inactive (close to 0).</p>
<p>The problem with both of these is that they suffered from a problem
called <a href="https://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_SSCI_2015/data/7560b423.pdf" class="external-link">model
saturation</a>. This is where very high or very low values are put into
the activation function, where the gradient of the line is almost flat.
This leads to very slow learning rates (it can take a long time to train
models with these activation functions).</p>
<p>Another very popular activation function that tries to tackle this is
the rectified linear unit (ReLU) function. This has 0 if the input is
negative (inactive) and just gives back the input if it is positive (a
measure of how active it is - the metaphor gets rather stretched here).
This is much faster at training and gives very good performance, but
still suffers model saturation on the negative side. Researchers have
tried to get round this with functions like ‘leaky’ ReLU, where instead
of returning 0, negative inputs are multiplied by a very small
number.</p>
<figure><img src="../fig/ActivationFunctions.png" alt="Activation functions" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="convolutional-neural-networks">Convolutional neural networks<a class="anchor" aria-label="anchor" href="#convolutional-neural-networks"></a>
</h2>
<hr class="half-width">
<p>Convolutional neural networks (CNNs) are a type of neural network
that especially popular for vision tasks such as image recognition. CNNs
are very similar to ordinary neural networks, but they have
characteristics that make them well suited to image processing.</p>
<p>Just like other neural networks, a CNN typically consists of an input
layer, hidden layers and an output layer. The layers of “neurons” have
learnable weights and biases, just like other networks.</p>
<p>What makes CNNs special? The name stems from the fact that the
architecture includes one or more convolutional layers. These layers
apply a mathematical operation called a “convolution” to extract
features from arrays such as images.</p>
<p>In a convolutional layer, a matrix of values referred to as a
“filter” or “kernel” slides across the input matrix (in our case, an
image). As it slides, values are multiplied to generate a new set of
values referred to as a “feature map” or “activation map”.</p>
<figure><img src="../fig/convolution_plotke.gif" alt="2D Convolution Animation by Michael Plotke" width="500" class="figure mx-auto d-block"></figure><p>Filters provide a mechanism for emphasising aspects of an input
image. For example, a filter may emphasise object edges. See <a href="https://setosa.io/ev/image-kernels/" class="external-link">setosa.io</a> for a visual
demonstration of the effect of different filters.</p>
</section><section><h2 class="section-heading" id="creating-a-convolutional-neural-network">Creating a convolutional neural network<a class="anchor" aria-label="anchor" href="#creating-a-convolutional-neural-network"></a>
</h2>
<hr class="half-width">
<p>Before training a convolutional neural network, we will first need to
define the architecture. We can do this using the Keras and Tensorflow
libraries.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># Create the architecture of our convolutional neural network, using</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># the tensorflow library</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> tensorflow.random <span class="im">import</span> set_seed</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense, Dropout, Conv2D, MaxPool2D, Input, GlobalAveragePooling2D</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># set random seed for reproducibility</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>set_seed(<span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a> </span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Our input layer should match the input shape of our images.</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># A CNN takes tensors of shape (image_height, image_width, color_channels)</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># We ignore the batch size when describing the input layer</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Our input images are 256 by 256, plus a single colour channel.</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>inputs <span class="op">=</span> Input(shape<span class="op">=</span>(<span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">1</span>))</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># Let's add the first convolutional layer</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(inputs)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co"># MaxPool layers are similar to convolution layers. </span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="co"># The pooling operation involves sliding a two-dimensional filter over each channel of feature map and selecting the max values.</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="co"># We do this to reduce the dimensions of the feature maps, helping to limit the amount of computation done by the network.</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="co"># We will add more convolutional layers, followed by MaxPool</span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">8</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a>x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">12</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">20</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">20</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>x <span class="op">=</span> MaxPool2D()(x)</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>x <span class="op">=</span> Conv2D(filters<span class="op">=</span><span class="dv">50</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="st">'same'</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a><span class="co"># Global max pooling reduces dimensions back to the input size</span></span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>x <span class="op">=</span> GlobalAveragePooling2D()(x)</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a><span class="co"># Finally we will add two "dense" or "fully connected layers".</span></span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a><span class="co"># Dense layers help with the classification task, after features are extracted.</span></span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a><span class="co"># Dropout is a technique to help prevent overfitting that involves deleting neurons.</span></span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a>x <span class="op">=</span> Dropout(<span class="fl">0.6</span>)(x)</span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a>x <span class="op">=</span> Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(x)</span>
<span id="cb1-48"><a href="#cb1-48" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" tabindex="-1"></a><span class="co"># Our final dense layer has a single output to match the output classes.</span></span>
<span id="cb1-50"><a href="#cb1-50" tabindex="-1"></a><span class="co"># If we had multi-classes we would match this number to the number of classes.</span></span>
<span id="cb1-51"><a href="#cb1-51" tabindex="-1"></a>outputs <span class="op">=</span> Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)(x)</span>
<span id="cb1-52"><a href="#cb1-52" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" tabindex="-1"></a><span class="co"># Finally, we will define our network with the input and output of the network</span></span>
<span id="cb1-54"><a href="#cb1-54" tabindex="-1"></a>model <span class="op">=</span> Model(inputs<span class="op">=</span>inputs, outputs<span class="op">=</span>outputs)</span></code></pre>
</div>
<p>We can view the architecture of the model:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>model.summary()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Model: "model_39"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 input_9 (InputLayer)        [(None, 256, 256, 1)]     0

 conv2d_59 (Conv2D)          (None, 256, 256, 8)       80

 max_pooling2d_50 (MaxPoolin  (None, 128, 128, 8)      0
 g2D)

 conv2d_60 (Conv2D)          (None, 128, 128, 8)       584

 max_pooling2d_51 (MaxPoolin  (None, 64, 64, 8)        0
 g2D)

 conv2d_61 (Conv2D)          (None, 64, 64, 12)        876

 max_pooling2d_52 (MaxPoolin  (None, 32, 32, 12)       0
 g2D)

 conv2d_62 (Conv2D)          (None, 32, 32, 12)        1308

 max_pooling2d_53 (MaxPoolin  (None, 16, 16, 12)       0
 g2D)

 conv2d_63 (Conv2D)          (None, 16, 16, 20)        6020

 max_pooling2d_54 (MaxPoolin  (None, 8, 8, 20)         0
 g2D)

 conv2d_64 (Conv2D)          (None, 8, 8, 20)          10020

 max_pooling2d_55 (MaxPoolin  (None, 4, 4, 20)         0
 g2D)

 conv2d_65 (Conv2D)          (None, 4, 4, 50)          25050

 global_average_pooling2d_8   (None, 50)               0
 (GlobalAveragePooling2D)

 dense_26 (Dense)            (None, 128)               6528

 dropout_8 (Dropout)         (None, 128)               0

 dense_27 (Dense)            (None, 32)                4128

 dense_28 (Dense)            (None, 1)                 33

=================================================================
Total params: 54,627
Trainable params: 54,627
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Dense layers, also known as fully connected layers, are an important
building block in most neural network architectures. In a dense layer,
each neuron is connected to every neuron in the preceeding layer.</li>
<li>Dropout is a method that helps to prevent overfitting by temporarily
removing neurons from the network.</li>
<li>The Rectified Linear Unit (ReLU) is an activation function that
outputs an input if it is positive, and outputs zero if it is not.</li>
<li>Convolutional neural networks are typically used for imaging
tasks.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-05-train_eval"><p>Content from <a href="05-train_eval.html">Training and evaluation</a></p>
<hr>
<p>Last updated on 2025-05-21 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/05-train_eval.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do I train a neural network?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Train a convolutational neural network for classification.</li>
<li>Evalute the network’s performance on a test set.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="compile-and-train-your-model">Compile and train your model<a class="anchor" aria-label="anchor" href="#compile-and-train-your-model"></a>
</h2>
<hr class="half-width">
<p>Now that the model architecture is complete, it is ready to be
compiled and trained! The distance between our predictions and the true
values is the error or “loss”. The goal of training is to minimise this
loss.</p>
<p>Through training, we seek an optimal set of model parameters. Using
an optimization algorithm such as gradient descent, our model weights
are iteratively updated as each batch of data is processed.</p>
<p>Batch size is the number of training examples processed before the
model parameters are updated. An epoch is one complete pass through all
of the training data. In an epoch, we use all of the training examples
once.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> optimizers</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.callbacks <span class="im">import</span> ModelCheckpoint</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co"># Define the network optimization method. </span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Adam is a popular gradient descent algorithm</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># with adaptive, per-parameter learning rates.</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>custom_adam <span class="op">=</span> optimizers.Adam()</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># Compile the model defining the 'loss' function type, optimization and the metric.</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>, optimizer<span class="op">=</span>custom_adam, metrics<span class="op">=</span>[<span class="st">'acc'</span>])</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="co"># Save the best model found during training</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>checkpointer <span class="op">=</span> ModelCheckpoint(filepath<span class="op">=</span><span class="st">'best_model.keras'</span>, monitor<span class="op">=</span><span class="st">'val_loss'</span>,</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>                               verbose<span class="op">=</span><span class="dv">1</span>, save_best_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># Now train our network!</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co"># steps_per_epoch = len(dataset_train)//batch_size</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>hist <span class="op">=</span> model.fit(datagen.flow(dataset_train, labels_train, batch_size<span class="op">=</span><span class="dv">32</span>), </span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>                 steps_per_epoch<span class="op">=</span><span class="dv">15</span>, </span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>                 epochs<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>                 validation_data<span class="op">=</span>(dataset_val, labels_val), </span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>                 callbacks<span class="op">=</span>[checkpointer])</span></code></pre>
</div>
<p>We can now plot the results of the training. “Loss” should drop over
successive epochs and accuracy should increase.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'loss'</span>], <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'train loss'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'val_loss'</span>], <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'val loss'</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>plt.show()</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'acc'</span>], <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'train accuracy'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>plt.plot(hist.history[<span class="st">'val_acc'</span>], <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'val accuracy'</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>plt.ylabel(<span class="st">'Accuracy'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">'lower right'</span>)</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<figure><img src="../fig/training_curves.png" alt="Training curves" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="evaluating-your-model-on-the-held-out-test-set">Evaluating your model on the held-out test set<a class="anchor" aria-label="anchor" href="#evaluating-your-model-on-the-held-out-test-set"></a>
</h2>
<hr class="half-width">
<p>In this step, we present the unseen test dataset to our trained
network and evaluate the performance.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> load_model </span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># Open the best model saved during training</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>best_model <span class="op">=</span> load_model(<span class="st">'best_model.keras'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">Neural network weights updated to the best epoch.'</span>)</span></code></pre>
</div>
<p>Now that we’ve loaded the best model, we can evaluate the accuracy on
our test data.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># We use the evaluate function to evaluate the accuracy of our model in the test group</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy in test group: </span><span class="sc">{</span>best_model<span class="sc">.</span>evaluate(dataset_test, labels_test, verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Accuracy in test group: 0.80</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>During the training process we iteratively update the model to
minimise error.</li>
</ul>
</div>
</div>
</div>
</section></section><section id="aio-06-explainability"><p>Content from <a href="06-explainability.html">Explainability</a></p>
<hr>
<p>Last updated on 2025-05-05 |

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/episodes/06-explainability.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is a saliency map?</li>
<li>What aspects of an image contribute to predictions?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Review model performance with saliency maps.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="explainability">Explainability<a class="anchor" aria-label="anchor" href="#explainability"></a>
</h2>
<hr class="half-width">
<p>If a model is making a prediction, many of us would like to know how
the decision was reached. Saliency maps - and related approaches - are a
popular form of explainability for imaging models.</p>
<p>Saliency maps use color to illustrate the extent to which a region of
an image contributes to a given decision. Let’s plot some saliency maps
for our model:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="co"># !pip install tf_keras_vis</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.gradcam <span class="im">import</span> Gradcam</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.gradcam_plus_plus <span class="im">import</span> GradcamPlusPlus</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.scorecam <span class="im">import</span> Scorecam</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="im">from</span> tf_keras_vis.utils.scores <span class="im">import</span> CategoricalScore</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># Select two differing explainability algorithms</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>gradcam <span class="op">=</span> GradcamPlusPlus(best_model, clone<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>scorecam <span class="op">=</span> Scorecam(best_model, clone<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="kw">def</span> plot_map(cam, classe, prediction, img):</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="co">    Plot the image.</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>    axes[<span class="dv">0</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>    axes[<span class="dv">1</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>    heatmap <span class="op">=</span> np.uint8(cm.jet(cam[<span class="dv">0</span>])[..., :<span class="dv">3</span>] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>    i <span class="op">=</span> axes[<span class="dv">1</span>].imshow(heatmap, cmap<span class="op">=</span><span class="st">"jet"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>    fig.colorbar(i)</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>    plt.suptitle(<span class="st">"Class: </span><span class="sc">{}</span><span class="st">. Pred = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(classe, prediction))</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># Plot each image with accompanying saliency map</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="cf">for</span> image_id <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a>    SEED_INPUT <span class="op">=</span> dataset_test[image_id]</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a>    CATEGORICAL_INDEX <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>    layer_idx <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>    penultimate_layer_idx <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>    class_idx  <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a>    cat_score <span class="op">=</span> labels_test[image_id]</span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a>    cat_score <span class="op">=</span> CategoricalScore(CATEGORICAL_INDEX)</span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a>    cam <span class="op">=</span> gradcam(cat_score, SEED_INPUT, </span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a>                  penultimate_layer <span class="op">=</span> penultimate_layer_idx,</span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a>                  normalize_cam<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a>    </span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a>    <span class="co"># Display the class</span></span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a>    _class <span class="op">=</span> <span class="st">'normal'</span> <span class="cf">if</span> labels_test[image_id] <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'effusion'</span></span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a>    _prediction <span class="op">=</span> best_model.predict(dataset_test[image_id][np.newaxis, :, ...], verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a>    </span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a>    plot_map(cam, _class, _prediction[<span class="dv">0</span>][<span class="dv">0</span>], SEED_INPUT)</span></code></pre>
</div>
<figure><img src="../fig/saliency.png" alt="Saliency maps" width="600" class="figure mx-auto d-block"></figure></section><section><h2 class="section-heading" id="sanity-checks-for-saliency-maps">Sanity checks for saliency maps<a class="anchor" aria-label="anchor" href="#sanity-checks-for-saliency-maps"></a>
</h2>
<hr class="half-width">
<p>While saliency maps may offer us interesting insights about regions
of an image contributing to a model’s output, there are suggestions that
this kind of visual assessment can be misleading. For example, the
following abstract is from a paper entitled “<a href="https://arxiv.org/abs/1810.03292" class="external-link">Sanity Checks for Saliency
Maps</a>”:</p>
<blockquote>
<p>Saliency methods have emerged as a popular tool to highlight features
in an input deemed relevant for the prediction of a learned model.
Several saliency methods have been proposed, often guided by visual
appeal on image data. … Through extensive experiments we show that some
existing saliency methods are independent both of the model and of the
data generating process. Consequently, methods that fail the proposed
tests are inadequate for tasks that are sensitive to either data or
model, such as, finding outliers in the data, explaining the
relationship between inputs and outputs that the model learned, and
debugging the model.</p>
</blockquote>
<p>There are multiple methods for producing saliency maps to explain how
a particular model is making predictions. The method we have been using
is called GradCam++, but how does this method compare to another? Use
this code to compare GradCam++ with ScoreCam.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">def</span> plot_map2(cam1, cam2, classe, prediction, img):</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co">    Plot the image.</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">14</span>, <span class="dv">5</span>))</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    axes[<span class="dv">0</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    axes[<span class="dv">1</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    axes[<span class="dv">2</span>].imshow(np.squeeze(img), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>    heatmap1 <span class="op">=</span> np.uint8(cm.jet(cam1[<span class="dv">0</span>])[..., :<span class="dv">3</span>] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>    heatmap2 <span class="op">=</span> np.uint8(cm.jet(cam2[<span class="dv">0</span>])[..., :<span class="dv">3</span>] <span class="op">*</span> <span class="dv">255</span>)</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>    i <span class="op">=</span> axes[<span class="dv">1</span>].imshow(heatmap1, cmap<span class="op">=</span><span class="st">"jet"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>    j <span class="op">=</span> axes[<span class="dv">2</span>].imshow(heatmap2, cmap<span class="op">=</span><span class="st">"jet"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>    fig.colorbar(i)</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>    plt.suptitle(<span class="st">"Class: </span><span class="sc">{}</span><span class="st">. Pred = </span><span class="sc">{}</span><span class="st">"</span>.<span class="bu">format</span>(classe, prediction))</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co"># Plot each image with accompanying saliency map</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="cf">for</span> image_id <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>    SEED_INPUT <span class="op">=</span> dataset_test[image_id]</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>    CATEGORICAL_INDEX <span class="op">=</span> [<span class="dv">0</span>]</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>    layer_idx <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>    penultimate_layer_idx <span class="op">=</span> <span class="dv">13</span></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>    class_idx  <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>    cat_score <span class="op">=</span> labels_test[image_id]</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>    cat_score <span class="op">=</span> CategoricalScore(CATEGORICAL_INDEX)</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>    cam <span class="op">=</span> gradcam(cat_score, SEED_INPUT, </span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>                  penultimate_layer <span class="op">=</span> penultimate_layer_idx,</span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>                  normalize_cam<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>    cam2 <span class="op">=</span> scorecam(cat_score, SEED_INPUT, </span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a>                  penultimate_layer <span class="op">=</span> penultimate_layer_idx,</span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a>                  normalize_cam<span class="op">=</span><span class="va">True</span></span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a>                  )</span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a>    </span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a>    <span class="co"># Display the class</span></span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a>    _class <span class="op">=</span> <span class="st">'normal'</span> <span class="cf">if</span> labels_test[image_id] <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">'effusion'</span></span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a>    _prediction <span class="op">=</span> best_model.predict(dataset_test[image_id][np.newaxis, : ,...], verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>    </span>
<span id="cb2-39"><a href="#cb2-39" tabindex="-1"></a>    plot_map2(cam, cam2, _class, _prediction[<span class="dv">0</span>][<span class="dv">0</span>], SEED_INPUT)</span></code></pre>
</div>
<p>Some of the time these methods largely agree:</p>
<figure><img src="../fig/saliency-agreement.png" alt="saliency_agreement" class="figure mx-auto d-block"></figure><p>But some of the time they disagree wildly:</p>
<figure><img src="../fig/saliency-disagreement.png" alt="saliency_disagreement" class="figure mx-auto d-block"></figure><p>This raises the question, should these algorithms be used at all?</p>
<p>This is part of a larger problem with explainability of complex
models in machine learning. The generally accepted answer is to know
<strong>how your model works</strong> and to know <strong>how your
explainability algorithm works</strong> as well as to <strong>understand
your data</strong>.</p>
<p>With these three pieces of knowledge it should be possible to
identify algorithms appropriate for your task, and to understand any
shortcomings in their approaches.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Saliency maps are a popular form of explainability for imaging
models.</li>
<li>Saliency maps should be used cautiously.</li>
</ul>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/machine-learning-neural-python/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/machine-learning-neural-python/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:tpollard@mit.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.12" class="external-link">sandpaper (0.16.12)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.6" class="external-link">varnish (1.0.6)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://carpentries-incubator.github.io/machine-learning-neural-python/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/machine-learning-neural-python/instructor/aio.html",
  "identifier": "https://carpentries-incubator.github.io/machine-learning-neural-python/instructor/aio.html",
  "dateCreated": "2021-10-25",
  "dateModified": "2025-05-21",
  "datePublished": "2025-05-21"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

